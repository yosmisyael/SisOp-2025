Name: Misyael Yosevian Wiarda

NRP: 3124500039

Class: 1 D3 IT B

Dosen Pengajar: Dr. Ferry Astika Saputra ST, M.Sc

# Appendix A Summary: Influential Operating Systems

## A.1

The concept of feature migration highlights how operating system functionalities, initially conceived for large mainframe systems, progressively trickle down to smaller devices like microcomputers and even handhelds. This trend underscores the enduring relevance of fundamental OS principles across a spectrum of computing platforms. A prominent illustration of this is the evolution of features from the MULTICS operating system, designed for mainframes, to the UNIX system, tailored for minicomputers, and subsequently to microcomputer operating systems such as Windows, macOS, and Linux, ultimately finding their way into PDAs. This migration demonstrates the dynamic nature of OS development, where innovations from high-capacity systems are adapted and integrated into increasingly compact and portable devices.

## A.2 Early Systems

Before 1940s, computers were fixed-task machines, but the advent of the stored-program concept by Turing and von Neumann revolutionized computing, leading to the development of general-purpose machines like the Manchester Mark 1 and the commercial model Ferranti Mark 1. These early systems were enormous, console-operated, and required manual program loading via switches, paper tape, or punched cards. Programmers directly executed and debugged programs from the console, monitoring execution through display lights, with output limited to printed or punched media; essentially, these systems lacked an operating system, with the programmer fulfilling the roles of operator and system administrator.

### A.2.1 Dedicated Computer Systems

The progression of computer systems saw the introduction of peripherals like card readers, line printers, and magnetic tapes, alongside software tools such as assemblers, loaders, and linkers, which collectively simplified programming. Reusable function libraries and device drivers, abstracting complex I/O operations, became essential. However, the execution of a job was a multi-stage process involving loading and unloading various tapes and cards, including the FORTRAN compiler, assembler, and object program, resulting in significant setup time. Errors at any stage could necessitate restarting the entire process. This setup overhead, coupled with manual console operation, led to considerable CPU idle time, a critical issue given the high cost and scarcity of computers. To maximize return on investment, high CPU utilization became paramount, driving the need to minimize idle time and streamline job execution.

### A.2.2 Shared Computer Systems

To address the inefficiencies of dedicated computer systems, shared computer systems emerged, introducing professional operators to replace programmers in machine operation, thereby reducing setup time and streamlining job transitions. Jobs with similar requirements were batched together to minimize setup overhead, but manual intervention remained necessary for job sequencing and error handling. Automatic job sequencing was then implemented through resident monitors, rudimentary operating systems that automated job transitions using control cards to instruct the system on program execution. These monitors comprised a control-card interpreter, a loader, and device drivers, facilitating automatic job sequencing and reducing human intervention. Despite these advancements, the inherent speed disparity between the fast CPU and slow mechanical I/O devices resulted in significant CPU idle time, a problem that persisted and worsened as CPU speeds increased more rapidly than I/O device speeds.

### A.2.3 Overlapped I/O

To mitigate the I/O bottleneck, early systems transitioned from slow card readers and line printers to faster magnetic tape units, employing off-line operations where cards were copied to tape and output written to tape for later printing. This shift significantly improved CPU utilization by decoupling it from the slow peripheral speeds. However, it introduced delays in job execution due to the sequential nature of tape processing. The advent of disk systems further revolutionized I/O handling by enabling random access, allowing for simultaneous reading and writing operations. Spooling, or simultaneous peripheral operation on-line, emerged as a key technique, utilizing the disk as a large buffer to overlap I/O operations with CPU processing. This approach not only enhanced system performance by keeping both the CPU and I/O devices busy but also paved the way for multiprogramming, a fundamental concept in modern operating systems. Spooling extended to remote data processing, where data was transmitted for off-site printing or input, further optimizing system efficiency.

## A.3 Atlas

The Atlas operating system, developed in the late 1950s and early 1960s at the University of Manchester, significantly influenced modern OS design by introducing features like device drivers and system calls via extra codes. As a batch system with spooling, Atlas optimized job scheduling based on peripheral device availability. Its most notable innovation was memory management, employing demand paging to utilize a small core memory as a cache for a larger drum memory. Addressing a vast virtual space of 1 million words with a 48-bit word architecture, Atlas divided memory into 512-word pages, managing virtual-to-physical address mapping with an associative memory. A predictive page-replacement algorithm, utilizing reference bits and historical access patterns, minimized page faults by prioritizing pages deemed no longer in use or those with the longest anticipated reuse intervals, reflecting an early attempt to optimize memory usage through sophisticated predictive techniques.

## A.4 XDS-940

The XDS-940 operating system, developed at the University of California, Berkeley in the early 1960s, introduced time-sharing and utilized paging for memory management, albeit for relocation rather than demand paging, differing from the Atlas system. With a virtual memory of 16 KB words and a physical memory of 64 KB words, it facilitated the concurrent execution of multiple user processes, enhancing efficiency through page sharing for read-only reentrant code. Processes were swapped between memory and a drum as needed. **The system, built on a modified XDS-930, incorporated user-monitor modes and privileged instructions to protect system resources.**^ 1 ^ A system-call instruction enabled the creation of resources like files, which were allocated in 256-word blocks on the drum, managed by a bitmap and indexed blocks. Furthermore, the XDS-940 system offered system calls for process management, allowing the creation, initiation, suspension, and termination of subprocesses, forming a hierarchical tree structure and enabling inter-process communication and synchronization through shared memory.

## A.5 THE
The THE operating system, developed at the Technische Hogeschool in Eindhoven during the mid-1960s, distinguished itself through its elegant layered architecture and the use of concurrent processes synchronized with semaphores, operating on a Dutch EL X8 computer with 32 KB of 27-bit words. Unlike the dynamic process creation of the XDS-940, THE employed a static set of cooperating processes, including five user processes for job execution. A priority-based CPU scheduling algorithm, favoring I/O-bound and new processes, was implemented. Memory management, constrained by hardware limitations, utilized a software paging scheme with an LRU replacement strategy, facilitated by the Algol compiler and a 512 KB drum backing store. Deadlock control was addressed using the bankerâ€™s algorithm. Related to THE, the Venus system also featured a layered design and semaphore-based synchronization but enhanced performance through microcode implementation of lower levels and adopted a paged-segmented memory approach for time-sharing rather than batch processing.

## A.6 RC 4000
The RC 4000 system, developed in the late 1960s for the Danish 4000 computer, focused on creating a foundational operating-system kernel rather than a specific batch or time-sharing system. This kernel supported concurrent processes and utilized a round-robin CPU scheduler, emphasizing inter-process communication through a message system. Processes exchanged fixed-size messages stored in a common buffer pool, with each process having a FIFO message queue. 2  The kernel provided atomic primitives for sending, waiting for, and answering messages, and additional primitives for flexible message handling. I/O devices were treated as processes, with device drivers converting interrupts and registers into messages, enabling processes to interact with peripherals via message exchange. This design prioritized a modular, message-driven approach to operating system construction.

## A.7 CTSS
The Compatible Time-Sharing System (CTSS), developed at MIT and first operational in 1961, was a pioneering experimental time-sharing system implemented on an IBM 7090, supporting up to 32 interactive users. It provided users with interactive commands for file manipulation and program compilation/execution via terminals. Utilizing a 32 KB memory, with 5 KB reserved for the monitor, CTSS employed a multilevel-feedback-queue CPU scheduling algorithm, where time quanta increased exponentially with queue levels. Programs were swapped between memory and a fast drum, with initial queue placement determined by program size to minimize swap overhead. CTSS demonstrated the viability and convenience of time-sharing, significantly influencing the development of subsequent time-sharing systems, including MULTICS.

## A.8 MULTICS
MULTICS, developed from 1965 to 1970 at MIT, was envisioned as a time-sharing utility, extending the concepts of CTSS to create a large-scale, continuously running system with a vast shared file system. Designed for the GE 645, a modified GE 635, MULTICS implemented paged-segmentation memory hardware, utilizing an 18-bit segment number and 16-bit word offset for virtual addressing, with segments paged in 1 KB words and employing the second-chance page-replacement algorithm. The system integrated the file system with the segmented virtual address space, addressing segments by file names within a multilevel tree structure. CPU scheduling utilized a multilevel feedback queue, while protection was managed through access lists and protection rings. Written primarily in PL/1, comprising approximately 300,000 lines of code, MULTICS was designed to support multiprocessor configurations, enabling continuous operation during CPU maintenance.

## A.9 IBM OS/360
IBM's OS/360, introduced in the mid-1960s, aimed to unify IBM's diverse computer line with a single operating system, but its attempt to cater to all needs resulted in a complex and inefficient system. Its file system was cumbersome, memory management was hindered by architectural limitations preventing dynamic relocation, and the massive assembly language codebase led to high overhead and persistent errors. The transition to the IBM/370 architecture brought virtual memory, leading to variations like OS/VS1, OS/VS2, and MVS, which gradually improved memory handling. Despite efforts to create a large-scale time-sharing system with TSS/360, it failed due to its size and slowness, leading to the adoption of TSO and CMS for time-sharing on IBM systems. Both TSS/360 and MULTICS, despite their ambitious designs, struggled with complexity and size, and were ultimately overtaken by the rise of minicomputers, workstations, and personal computers, which shifted computing power closer to individual users.

## A.10 TOPS-20
DEC's TOPS-20, evolving from BBN's TENEX, marked a significant advancement in time-sharing systems. Initially developed around 1970 on a modified DEC PDP-10 with added hardware paging, TENEX was a general-purpose time-sharing OS that leveraged virtual memory. DEC's acquisition of TENEX led to the DECSYSTEM-20 and TOPS-20, which featured an advanced command-line interpreter with user-friendly help features. This, combined with the DECSYSTEM-20's powerful hardware and competitive pricing, made it the preeminent time-sharing system of its era. However, in 1984, DEC shifted its focus to 32-bit VAX systems and VMS, discontinuing the 36-bit PDP-10 line.

## A.11 CP/M and MS-DOS 
CP/M and MS-DOS represent significant milestones in the development of personal computer operating systems. CP/M, created by Gary Kindall of Digital Research, Inc., emerged as an early standard for 1970s hobbyist computers, running on the 8-bit Intel 8080 and supporting single-program execution within a 64 KB memory limit. Its command interpreter drew inspiration from systems like DEC's TOPS-10. Subsequently, when IBM entered the PC market, they commissioned Bill Gates and Microsoft to develop MS-DOS for the 16-bit Intel 8086. MS-DOS, while similar to CP/M, featured an expanded set of built-in commands, also largely influenced by TOPS-10. Dominating the personal computer OS landscape from 1981 to 2000, MS-DOS supported up to 640 KB of memory, with workarounds for extended memory, but lacked modern OS features such as protected memory.

## A.12 Macintosh Operating Systems and Windows
The transition to 16-bit CPUs enabled the development of more advanced and user-friendly personal computer operating systems. Apple's Macintosh, launched in 1984, pioneered the graphical user interface (GUI) for home users, utilizing a mouse for interaction and including numerous utility programs. However, its initial exclusivity to Apple hardware led to it being overtaken by Microsoft Windows, which, starting with version 1.0 in 1985, was licensed to run on a multitude of computers. As microprocessor technology advanced to 32-bit chips, operating systems like Windows and Mac OS incorporated features previously exclusive to mainframes and minicomputers. This evolution transformed personal computers into powerful and versatile tools, ultimately contributing to the decline of minicomputers. The ongoing competition between Windows and Mac OS continues to drive innovation in features, usability, and application functionality, while Linux's popularity grows among technical users and within initiatives like the OLPC project.

## A.13 Mach
The Mach operating system, originating from Carnegie Mellon University's Accent, aimed to emulate 4.3 BSD UNIX, support modern computing models, and simplify kernel design. Mach's development, beginning in the mid-1980s, progressed from BSD UNIX, with gradual component replacement, leading to releases supporting various hardware platforms. Mach 3 introduced a microkernel architecture, moving BSD code to user-mode servers, enabling multi-OS support. The Open Software Foundation's adoption of Mach 2.5 for OSF/1 and its use in NeXT's workstation highlighted its significance. Mach's design emphasized multiprocessing, utilizing lightweight threads and message-based communication for efficient and flexible operation. Despite its influence, pure Mach implementations are rare today, with its legacy primarily living on in Apple's XNU kernel, which powers macOS and iOS. XNU combines a Mach core with BSD APIs, demonstrating Mach's enduring impact on modern operating system design.

## A.14 Capability-based Systems -- Hydra and CAP
### A.14.1 Hydra
Hydra is a capability-based protection system designed for flexibility, allowing both system-defined and user-defined access rights. It manages object operations procedurally, where procedures themselves are objects accessed via capabilities. User-defined procedures are registered with Hydra to define object types and auxiliary rights, enabling fine-grained control over access on an instance and process basis. Hydra's rights amplification feature allows trusted procedures to operate with elevated privileges, independent of the caller's rights, but restricts this amplification to specific types and procedures to maintain security. It dynamically adjusts access rights during procedure calls to ensure data abstraction integrity. Hydra addresses the "mutually suspicious subsystems" problem by providing mechanisms for secure interaction between services and users, protecting both user data and service resources. Subsystems are built atop Hydra's kernel, using its standard protection mechanisms, and programmers can access Hydra's extensive library of system procedures either directly or through program translators.

### A.14.2 Cambridge CAP Systems
The Cambridge CAP system employs a distinct approach to capability-based protection, utilizing data and software capabilities. Data capabilities, interpreted by microcode, grant standard read, write, and execute rights to storage segments. Software capabilities, protected but not interpreted by microcode, are managed by privileged procedures, enabling application programmers to implement custom protection schemes. A specific form of rights amplification allows these procedures to temporarily access software capability contents, subject to type verification, ensuring security. The interpretation of software capabilities is delegated to subsystems, facilitating diverse protection policies. While this flexibility allows for efficient implementation of abstract resource protection, it necessitates that subsystem designers possess a deep understanding of protection principles and techniques, as the system does not provide a pre-built library of procedures.

## Other Systems
Beyond the systems already discussed, numerous other operating systems exist, each with unique characteristics. For instance, the MCP operating system, designed for Burroughs computers, was groundbreaking for being the first to be written in a system programming language and for its support of segmentation and multiple CPUs. Similarly, the SCOPE operating system for the CDC 6600 showcased sophisticated coordination and synchronization mechanisms for its multi-CPU architecture. Throughout computing history, many operating systems have emerged, served their purpose, and eventually been superseded by newer systems offering enhanced features, hardware compatibility, usability, or marketing. This cycle of innovation and replacement is expected to continue in the future, as technology and user needs evolve.
